# Default values for anchore_engine chart.

# The configuration for the API service, which must be reachable inside the cluster by other workers and users
service:
  type: ClusterIP
  ports:
    analyzer: 8084
    extApi: 8228
    simplequeue: 8083
    catalog: 8082
    policyEngine: 8087
    kubernetesWebhook: 8338

image:
  # Can use 'latest' but not recommended
  repository: "docker.io/anchore/anchore-engine"
  tag: "dev"
  #pullPolicy: Always

# Used to create Ingress record (should used with service.type: ClusterIP or NodePort depending on platform)
ingress:
  enabled: false
  annotations: {}
    # kubernetes.io/ingress.allow-http: False
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: true
  path: /
  # You can bound on specific hostnames
  # hosts:
  #   - chart-example.local
  tls: []
  # Secrets must be manually created in the namespace.
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

# Dependency on Postgresql, configure here
anchore-db:
  enabled: true
  postgresUser: anchoreengine
  postgresPassword: anchore-postgres,123
  postgresDatabase: anchore

  # Use this config if you set enabled=False and want to specify an external (already existing) postres deployment for use.
  # Set this to the host and port. eg. mypostgres.myserver.io:5432
  externalEndpoint: Null

# Global configuration shared by both core and worker
global:
  # Set where default configs are placed at startup. This must be a writable location for the pod.
  configDir: /anchore_service_config
  logLevel: INFO

  # Enable prometheus metrics
  enableMetrics: false

  default_admin_password: foobar
  default_admin_email: admin@myemail.com

  dbConfig:
    timeout: 120
    # Use ssl, but the default postgresql config in helm's stable repo does not support ssl on server side, so this should be set for external dbs only for the time being
    ssl: false
    connectionPoolSize: 30
    connectionPoolMaxOverflow: 100

  # Cleanup local images used during analysis, defaults to True. If set to false, images will remain on analyzers after analysis.
  cleanupImages: true

  # If True, if a user adds an ECR registry with username = awsauto then the system will look for an instance profile to use for auth against the registry
  allowECRUseIAMRole: false

  internalServicesSslEnabled: false
  internalServicesSslVerifyCerts: false

  # Configure webhook outputs here. The service provides these webhooks for notifying external systems of updates
  webhooks:
    enabled: False
    config:
      # User and password to be set (using HTTP basic auth) on all webhook calls if necessary
      webhook_user: null
      webhook_password: null
      ssl_verify: true

      # Endpoint for general notification delivery. These events are image/tag updates etc. This is globally configured
      # and updates for all users are sent to the same host but with a different path for each user.
      general: {}
        # url: "http://somehost:9090/<notification_type>/<userId>"

      # Endpoint and credentials for policy evaluation delivery
      policy_eval: {}
        # url: "http://somehost:9090/policy_eval/<userId>"
        # user: null
        # password: null

      # Endpoint for fatal system errors to be delivery
      error_event: {}
        # url: 'http://somehost:9090/error_event/'

  # Intervals to run specific events on (seconds)
  cycleTimers:
    # Interval to check for an update to a tag
    image_watcher: 3600
    # Interval to re-run a policy eval on a tag
    policy_eval: 3600
    # Interval to run a feed sync to get latest cve data
    feed_sync: 14400
    # Interval workers check the queue
    analyzer_queue: 1
    # Interval notifications will be processed for state changes
    notifications: 30
    # Intervals service state updates are polled
    service_watcher: 15
    # Interval for policy bundle sync from anchore.io if enabled
    policy_bundle_sync: 300

# Configuration for the core engine service that serves the API
# The core service handles the user facing APIs and coordination of workers as well as storage interfaces for data
core:
  replicaCount: 1

  # Policy bundle sync enables the engine to download a policy bundle from anchore.io if you have an account there with a custom bundle. Requires providing your login credentials in the globalConfig.users.anchoreIOCredentials section
  policyBundleSyncEnabled: false

  ssl:
    # To use certs for TLS directly from the services, create a secret with keys that match the values fo certSecretKey and certSecretCert
    certSecret: null
    certSecretKeyName: "tls.key"
    certSecretCertName: "tls.crt"
    certDir: "/certs"

  # resources:
  #  limits:
  #    cpu: 100m
  #    memory: 6Gi
  #  requests:
  #    cpu: 100m
  #    memory: 4Gi

  ## Node labels for pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  annotations: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}

# Configuration for the analyzer pods that perform image analysis
# There may be many of these analyzers but best practice is to not have more than one per node since analysis
# is very IO intensive. Use of affinity/anti-affinity rules for scheduling the analyzers is future work.
analyzer:
  replicaCount: 1

  # The cycle timer is the interval between checks to the work queue for new jobs
  cycleTimerSeconds: 1

  # Controls the concurrency of the analyzer itself. Can be configured to process more than one task at a time, but it IO bound, so may not
  # necessarily be faster depending on hardware. Should test and balance this value vs. number of analyzers for your deployment cluster performance.
  concurrentTasksPerWorker: 1

  # The analysisVolume controls the mounting of an external volume for scratch space for image analysis. Generally speaking
  # you need to provision 3x the size of the largest image (uncompressed) that you want to analyze for this space.
  analysisScratchVolume:
    mountPath: /tmp
    details:
      emptyDir: {}

  # Configuration for ssl used for internal node communications between components
  ssl:
    certDir: "/certs"
    certSecret: null
    certSecretKeyName: "tls.key"
    certSecretCertName: "tls.crt"

  # resources:
  #  limits:
  #    cpu: 100m
  #    memory: 3Gi
  #  requests:
  #    cpu: 100m
  #    memory: 2Gi

  ## Node labels for pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  annotations: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}

catalog:
  replicaCount: 1

  # Configuration for ssl used for internal node communications between components
  ssl:
    certDir: "/certs"
    certSecret: null
    certSecretKeyName: "tls.key"
    certSecretCertName: "tls.crt"

    # Event log configuration
    events:
      notification:
        enabled: true
        # Send notifications for events with severity level that matches items in this list
        level:
          - error
          # - info
    archive:
      compression:
        enabled: true
        min_size_kbytes: 100
      storage_driver:
        # Valid storage driver names: 'db', 'localfs', 's3', 'swift'
        # It is strongly recommended to only use localfs for local testing using persisent volumes or a shared FS and generally not in a k8s deployment.
        name: db
        config: {}

        # Example S3 Configuration:
        # name: s3
        # config:
        #   # All objects are stored in a single bucket, defined here
        #   bucket: "anchore-engine-testing"
        #   # A prefix for keys in the bucket if desired (optional)
        #   prefix: "internaltest"
        #   # Create the bucket if it doesn't already exist
        #   create_bucket: False
            # Url only needed for non-AWS S3 implementations (e.g. minio). Otherwise, configure the region instead
        #   #url: "https://s3.amazonaws.com"
        #   # AWS region to connect to if 'url' not specified, if both are set, then 'url' has precedent
        #   region: us-west-2
        #   # For Auth can provide access/secret keys or use 'iamauto' which will use an instance profile or any credentials found in normal aws search paths/metadata service
        #   access_key: XXXX
        #   secret_key: YYYY
        #   iamauto: False

        # Example Minio configuration (basically same as s3 example):
        # name: s3
        # config:
        #   url: http://<minio url>:9000
        #   bucket: mybucket
        #   access_key: xxxxxx
        #   secret_key: yyyyyy
        #   create_bucket: True

        # Example Swift Configuration:
        # name: swift
        # config:
        #     # Config for swift has a few options, just add the keys and names as used to configure a swiftclient here. All are passed directly to the client impl.
        #     user: "test:tester"
        #     key: "testing"
        #     auth: "http://swift_ephemeral:8080/auth/v1.0"
        #     # The swift container where data will be stored
        #     container: "local_test_anchore"
        #     # Create the container if it is not already present
        #     create_container: False

  # resources:
  #  limits:
  #    cpu: 100m
  #    memory: 3Gi
  #  requests:
  #    cpu: 100m
  #    memory: 2Gi

  ## Node labels for pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  annotations: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}

policyEngine:
  replicaCount: 1

  # Configuration for ssl used for internal node communications between components
  ssl:
    certDir: "/certs"
    certSecret: null
    certSecretKeyName: "tls.key"
    certSecretCertName: "tls.crt"

  # resources:
  #  limits:
  #    cpu: 100m
  #    memory: 3Gi
  #  requests:
  #    cpu: 100m
  #    memory: 2Gi

  ## Node labels for pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  annotations: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}

kubernetesWebhook:
  replicaCount: 1

  # Configuration for ssl used for internal node communications between components
  ssl:
    certDir: "/certs"
    certSecret: null
    certSecretKeyName: "tls.key"
    certSecretCertName: "tls.crt"

  # resources:
  #  limits:
  #    cpu: 100m
  #    memory: 3Gi
  #  requests:
  #    cpu: 100m
  #    memory: 2Gi

  ## Node labels for pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  annotations: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}

simplequeue:
  replicaCount: 1

  # Configuration for ssl used for internal node communications between components
  ssl:
    certDir: "/certs"
    certSecret: null
    certSecretKeyName: "tls.key"
    certSecretCertName: "tls.crt"

  # resources:
  #  limits:
  #    cpu: 100m
  #    memory: 3Gi
  #  requests:
  #    cpu: 100m
  #    memory: 2Gi

  ## Node labels for pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  annotations: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}

enterpriseGlobal:
  enabled: true
  licenseSecretName: anchore-license
  imagePullSecretName: anchore-dockerhub-creds

  image:
    repository: docker.io/anchore/enterprise-dev
    tag: master
    #pullPolicy: Always

enterpriseRbac:
  enabled: true

  service:
    apiPort: 8229
    authPort: 8089

enterpriseUi:
  enabled: true
  replicaCount: 1
  # The (optional) `enable_proxy` key specifies whether to trust a reverse proxy
  # when setting secure cookies (via the `X-Forwarded-Proto` header). The value
  # must be a  Boolean, and defaults to `False` if unset. In addition, SSL must be
  # enabled for this to work.
  enableProxy: False
  # The (optional) `enable_ssl` key specifies if SSL is enabled in the web app
  # container. The value must be a Boolean, and defaults to `False` if unset.
  enableSsl: False
  # The (optional) `allow_shared_login` key specifies if a single set of user
  # credentials can be used to start multiple Anchore Enterprise UI sessions; for
  # example, by multiple users across different systems, or by a single user on a
  # single system across multiple browsers.
  #
  # When set to `False`, only one session per credential is permitted at a time,
  # and logging in will invalidate any other sessions that are using the same set
  # of credentials.
  #
  # If this property is unset, or is set to anything other than a Boolean, the web
  # service will default to `True`.
  #
  # Note that setting this property to `False` does not prevent a single session
  # from being viewed within multiple *tabs* inside the same browser.
  enableSharedLogin: True

  image:
    repository: docker.io/anchore/private_testing
    tag: anchore-ui-dev
    pullPolicy: Always

  service:
    type: ClusterIP
    httpPort: 80

  annotations: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}

enterpriseFeeds:
  enabled: true
  replicaCount: 1
  # Time delay in seconds between consecutive driver runs for processing data
  cycle_timer_seconds: 7200
  # Staging space for holding normalized output from drivers. Persisted using a bind mounted volume defined in docker-compose
  local_workspace: /workspace
  # Drivers process data from external sources and store normalized data in local_workspace. Processing large data sets
  # is a time consuming process for some drivers. To speed it up the container is shipped with pre-loaded data which is used
  # by default if local_workspace is empty.
  workspace_preload_disabled: False
    # Set True to not use pre-loaded data if local_workspace is empty. Drivers will generate normalized data from scratch
    # To load the workspace from a different location, uncomment and configure workspace_preload_file property to point to the tar.gz file
  workspace_preload_file: /workspace_preload/data.tar.gz
  # Configuration section for drivers collecting and processing feed data.
  # All drivers are enabled by default unless explicitly disabled. npm and gem drivers are explicitly disabled out of the box
  api_only: False
  npm_enabled: False
  gem_enabled: False
  # If gem_enabled is set to 'True' then gem_db_connect should be set to an accessible postgres endpoint. eg. mypostgres.myserver.io:5432
  gem_db_connect: Null
  centos_enabled: True
  debian_enabled: True
  ubuntu_enabled: True
  ol_enabled: True
  alpine_enabled: True
  snyk_enabled: True

  service:
    type: ClusterIP
    apiPort: 8448

  annotations: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}

# This section is used to configure the second postgres database instance for the enterprise feeds service.
# Only utilized if the enterpriseConfig.feeds.enabled is set to 'true'
feeds-db:
  enabled: true
  postgresUser: anchoreengine
  postgresPassword: anchore-postgres,123
  postgresDatabase: anchore-feeds

  # Use this config if you set enabled=False and want to specify an external (already existing) postres deployment for use.
  # Set this to the host and port. eg. mypostgres.myserver.io:5432
  externalEndpoint: Null
